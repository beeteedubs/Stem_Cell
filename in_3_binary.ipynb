{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.pooling import AveragePooling2D\n",
    "from keras.applications import VGG16\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "# from https://github.com/RohitSaha/VGG_Imagenet_Weights_GrayScale_Images/blob/master/convert_vgg_grayscale.py\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image, ImageEnhance\n",
    "from easydict import EasyDict\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DO NOT CHANGE-------------------------------------------------------------------------------------------\n",
    "\n",
    "# # creates new weights for a 1 channel image\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "# model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# # Block1_conv1 weights are of the format [3, 3, 3, 64] -> this is for RGB images\n",
    "# # For grayscale, format should be [3, 3, 1, 64]. Weighted average of the features has to be calculated across channels.\n",
    "# # RGB weights: Red 0.2989, Green 0.5870, Blue 0.1140\n",
    "\n",
    "# # getting weights of block1 conv1.\n",
    "# block1_conv1 = model.get_layer('block1_conv1').get_weights()\n",
    "# weights, biases = block1_conv1\n",
    "\n",
    "# # :weights shape = [3, 3, 3, 64] - (0, 1, 2, 3)\n",
    "# # convert :weights shape to = [64, 3, 3, 3] - (3, 2, 0, 1)\n",
    "# weights = np.transpose(weights, (3, 2, 0, 1))\n",
    "\n",
    "\n",
    "# kernel_out_channels, kernel_in_channels, kernel_rows, kernel_columns = weights.shape\n",
    "\n",
    "# # Dimensions : [kernel_out_channels, 1 (since grayscale), kernel_rows, kernel_columns]\n",
    "# grayscale_weights = np.zeros((kernel_out_channels, 1, kernel_rows, kernel_columns))\n",
    "\n",
    "# # iterate out_channels number of times\n",
    "# for i in range(kernel_out_channels):\n",
    "\n",
    "# \t# get kernel for every out_channel\n",
    "# \tget_kernel = weights[i, :, :, :]\n",
    "\n",
    "# \ttemp_kernel = np.zeros((3, 3))\n",
    "\n",
    "# \t# :get_kernel shape = [3, 3, 3]\n",
    "# \t# axis, dims = (0, in_channel), (1, row), (2, col)\n",
    "\n",
    "# \t# calculate weighted average across channel axis\n",
    "# \tin_channels, in_rows, in_columns = get_kernel.shape\n",
    "\n",
    "# \tfor in_row in range(in_rows):\n",
    "# \t\tfor in_col in range(in_columns):\n",
    "# \t\t\tfeature_red = get_kernel[0, in_row, in_col]\n",
    "# \t\t\tfeature_green = get_kernel[1, in_row, in_col]\n",
    "# \t\t\tfeature_blue = get_kernel[2, in_row, in_col]\n",
    "\n",
    "# \t\t\t# weighted average for RGB filter\n",
    "# \t\t\ttotal = (feature_red * 0.2989) + (feature_green * 0.5870) + (feature_blue * 0.1140)\n",
    "\n",
    "# \t\t\ttemp_kernel[in_row, in_col] = total\n",
    "\n",
    "\n",
    "# \t# :temp_kernel is a 3x3 matrix [rows x columns]\n",
    "# \t# add an axis at the end to specify in_channel as 1\n",
    "\n",
    "# \t# 2 ways of doing this,\n",
    "\n",
    "# \t# First: Add axis directly at the end of :temp_kernel to make its shape: [3, 3, 1], but this might be \n",
    "# \t# an issue when concatenating all feature maps\n",
    "\n",
    "# \t# Second: Add axis at the start of :temp_kernel to make its shape: [1, 3, 3] which is [in_channel, rows, columns]\n",
    "# \ttemp_kernel = np.expand_dims(temp_kernel, axis=0)\n",
    "\n",
    "# \t# Now, :temp_kernel shape is [1, 3, 3]\n",
    "\n",
    "# \t# Concat :temp_kernel to :grayscale_weights along axis=0\n",
    "# \tgrayscale_weights[i, :, :, :] = temp_kernel\n",
    "\n",
    "# # Dimension of :grayscale_weights is [64, 1, 3, 3]\n",
    "# # In order to bring it to tensorflow or keras weight format, transpose :grayscale_weights\n",
    "\n",
    "# # dimension, axis of :grayscale_weights = (out_channels: 0), (in_channels: 1), (rows: 2), (columns: 3)\n",
    "# # tf format of weights = (rows: 0), (columns: 1), (in_channels: 2), (out_channels: 3)\n",
    "\n",
    "# # Go from (0, 1, 2, 3) to (2, 3, 1, 0)\n",
    "# grayscale_weights = np.transpose(grayscale_weights, (2, 3, 1, 0)) # (3, 3, 1, 64)\n",
    "\n",
    "# # combine :grayscale_weights and :biases\n",
    "# new_block1_conv1 = [grayscale_weights, biases]\n",
    "\n",
    "\n",
    "# # Reconstruct the layers of VGG16 but replace block1_conv1 weights with :grayscale_weights\n",
    "\n",
    "# # get weights of all the layers starting from 'block1_conv2'\n",
    "# vgg16_weights = {}\n",
    "# for layer in model.layers[2:]:\n",
    "# \tif \"conv\" in layer.name:\n",
    "# \t\tvgg16_weights[\"200_\" + layer.name] = model.get_layer(layer.name).get_weights()\n",
    "\n",
    "# del model\n",
    "\n",
    "\n",
    "# # Custom build VGG16\n",
    "# input = Input(shape=(200, 200, 1), name='200_input')\n",
    "# # Block 1\n",
    "# x = Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(200, 200, 1), data_format=\"channels_last\", name='200_block1_conv1')(input)\n",
    "# x = Conv2D(64, (3, 3), activation='relu', padding='same', name='200_block1_conv2')(x)\n",
    "# x = MaxPooling2D((2, 2), strides=(2, 2), name='200_block1_pool')(x)\n",
    "\n",
    "# # Block 2\n",
    "# x = Conv2D(128, (3, 3), activation='relu', padding='same', name='200_block2_conv1')(x)\n",
    "# x = Conv2D(128, (3, 3), activation='relu', padding='same', name='200_block2_conv2')(x)\n",
    "# x = MaxPooling2D((2, 2), strides=(2, 2), name='200_block2_pool')(x)\n",
    "\n",
    "# # Block 3\n",
    "# x = Conv2D(256, (3, 3), activation='relu', padding='same', name='200_block3_conv1')(x)\n",
    "# x = Conv2D(256, (3, 3), activation='relu', padding='same', name='200_block3_conv2')(x)\n",
    "# x = Conv2D(256, (3, 3), activation='relu', padding='same', name='200_block3_conv3')(x)\n",
    "# x = MaxPooling2D((2, 2), strides=(2, 2), name='200_block3_pool')(x)\n",
    "\n",
    "# # Block 4\n",
    "# x = Conv2D(512, (3, 3), activation='relu', padding='same', name='200_block4_conv1')(x)\n",
    "# x = Conv2D(512, (3, 3), activation='relu', padding='same', name='200_block4_conv2')(x)\n",
    "# x = Conv2D(512, (3, 3), activation='relu', padding='same', name='200_block4_conv3')(x)\n",
    "# x = MaxPooling2D((2, 2), strides=(2, 2), name='200_block4_pool')(x)\n",
    "\n",
    "# # Block 5\n",
    "# x = Conv2D(512, (3, 3), activation='relu', padding='same', name='200_block5_conv1')(x)\n",
    "# x = Conv2D(512, (3, 3), activation='relu', padding='same', name='200_block5_conv2')(x)\n",
    "# x = Conv2D(512, (3, 3), activation='relu', padding='same', name='200_block5_conv3')(x)\n",
    "# x = MaxPooling2D((8, 8), strides=(8, 8), name='200_block5_pool')(x)\n",
    "\n",
    "# base_model = Model(inputs=input, outputs=x)\n",
    "\n",
    "# base_model.get_layer('200_block1_conv1').set_weights(new_block1_conv1)\n",
    "# for layer in base_model.layers[2:]:\n",
    "# \tif 'conv' in layer.name:\n",
    "# \t\tbase_model.get_layer(layer.name).set_weights(vgg16_weights[layer.name])\n",
    "\n",
    "# #base_model.summary()\n",
    "\n",
    "# #print base_model.get_layer('block3_conv2').get_weights()\n",
    "# base_model.save('vgg_grayscale_200.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change {args.dataset, np.array colors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-d\", \"--dataset\", type=str, default = 'Batches/testing',\n",
    "    help=\"path to input dataset\")\n",
    "ap.add_argument(\"-e\", \"--epochs\", type=int, default=25,\n",
    "    help=\"# of epochs to train our network for\")\n",
    "ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
    "    help=\"path to output loss/accuracy plot\")\n",
    "args = args = EasyDict({\n",
    "    \"dataset\": 'Batches/Test_Batch_1',\n",
    "    \"epochs\": 25,\n",
    "    'plot':'plot2_2582.png'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images for train, valid, and test: 5164\n"
     ]
    }
   ],
   "source": [
    "print ('Total number of images for train, valid, and test:', len([name for name in os.listdir('Batches/Test_Batch_1/Prolif')]) + \\\n",
    "        len([name for name in os.listdir('Batches/Test_Batch_1/Senescent')]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.2.0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2. __version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading images...\")\n",
    "imagePaths = list(paths.list_images(args[\"dataset\"]))\n",
    "data = []\n",
    "labels = []\n",
    "# loop over the image paths\n",
    "for imagePath in imagePaths:\n",
    "\n",
    "    # extract the class label from the filename\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    \n",
    "    image = cv2.imread(imagePath)\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "    f1_image = cv2.flip(image,0)    \n",
    "    labels.append(label)\n",
    "    data.append(f1_image)\n",
    "    \n",
    "    r1_image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "    labels.append(label)\n",
    "    data.append(r1_image)\n",
    "    f2_image = cv2.flip(r1_image,0)\n",
    "    labels.append(label)\n",
    "    data.append(f2_image)\n",
    "    \n",
    "    r2_image = cv2.rotate(r1_image, cv2.ROTATE_90_CLOCKWISE)\n",
    "    labels.append(label)\n",
    "    data.append(r2_image)\n",
    "    f3_image = cv2.flip(r2_image,0)\n",
    "    labels.append(label)\n",
    "    data.append(f3_image)\n",
    "    \n",
    "    r3_image = cv2.rotate(r2_image, cv2.ROTATE_90_CLOCKWISE)\n",
    "    labels.append(label)\n",
    "    data.append(r3_image)\n",
    "    f4_image = cv2.flip(r3_image,0)\n",
    "    labels.append(label)\n",
    "    data.append(f4_image)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# ------------------------------hto garbage---------------\n",
    "#     load the image, swap color channels, and resize it to be a fixed\n",
    "#     128x128 pixels while ignoring aspect ratio\n",
    "#     competing = Image.open(imagePath)\n",
    "#     compete = np.array(competing)\n",
    "#     compete = np.expand_dims(compete, axis = 2)\n",
    "#     print('type', type(imagePath))\n",
    "#     print(type(image))\n",
    "#     #image = Image.open(imagePath)\n",
    "#     print('image shape: ', np.array(image).shape)\n",
    "#     print('images dimesnions: ',image.ndim)\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     image = cv2.resize(image, (200, 200))\n",
    "#     print('image dimensions: ', np.array(image).shape)\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # update the data and labels lists, respectively\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "#     img = Image.open(imagePath)\n",
    "#     image = np.array(img)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41312 images points\n"
     ]
    }
   ],
   "source": [
    "print(len(labels), 'images points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bryan\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(labels)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "onehot_encoded\n",
    "#len(onehot_encoded)\n",
    "\n",
    "# convert the data and labels to NumPy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "# perform one-hot encoding on the labels\n",
    "lb = LabelBinarizer()\n",
    "labelss = lb.fit_transform(labels)\n",
    "#labelss = np_utils.to_categorical(labelss)\n",
    "\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, onehot_encoded,test_size=0.1, random_state=42)\n",
    "(trainX, valX, trainY, valY) = train_test_split(trainX, trainY, test_size = 0.1, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the training data augmentation object\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "trainAug = ImageDataGenerator()\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# initialize the validation/testing data augmentation object (which\n",
    "# we'll be adding mean subtraction to)\n",
    "valAug = ImageDataGenerator()\n",
    "# define the ImageNet mean subtraction (in RGB order) and set the\n",
    "# the mean subtraction value for each of the data augmentation\n",
    "# objects\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
    "# trainAug.mean = mean\n",
    "# valAug.mean = mean\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load VGG16, ensuring the head FC layer sets are left off, while at\n",
    "# the same time adjusting the size of the input image tensor to the\n",
    "# network\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
    "# \tinput_tensor=Input(shape=(200, 200, 1)))\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "baseModel = keras.applications.vgg16.VGG16(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=None,\n",
    "        input_shape=(200, 200, 3),\n",
    "        pooling=None,\n",
    "        classes=2)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# show a summary of the base model\n",
    "#print(\"[INFO] summary for base model...\")\n",
    "#print(baseModel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(4, 4))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(200, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "# place the head FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "# loop over all layers in the base model and freeze them so they will\n",
    "# *not* be updated during the first training process\n",
    "for layer in baseModel.layers:\n",
    "\tlayer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training head...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b4e274db2584>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m H = model.fit_generator(\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mtrainAug\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;31m#steps_per_epoch=30,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalAug\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mflow\u001b[1;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, subset)\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[0msave_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_prefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m         )\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, dtype)\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m             dtype=dtype)\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, dtype)\u001b[0m\n\u001b[0;32m    110\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_misc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_misc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compile our model (this needs to be done after our setting our\n",
    "# layers to being non-trainable)\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=1e-4)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "# train the head of the network for a few epochs (all other layers\n",
    "# are frozen) -- this will allow the new FC layers to start to become\n",
    "# initialized with actual \"learned\" values versus pure random\n",
    "print(\"[INFO] training head...\")\n",
    "\n",
    "H = model.fit_generator(\n",
    "\ttrainAug.flow(trainX, trainY, batch_size=100),\n",
    "\t#steps_per_epoch=30,\n",
    "\tvalidation_data=valAug.flow(testX, testY),\n",
    "\tvalidation_steps=10,\n",
    "\tepochs=args[\"epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=32)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "\tpredictions.argmax(axis=1), target_names=lb.classes_))\n",
    "# plot the training loss and accuracy\n",
    "N = args[\"epochs\"]\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(args[\"plot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = model.predict(valX)\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(valY, y_pred_class))\n",
    "print(metrics.confusion_matrix(valY, y_pred_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
